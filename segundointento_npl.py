# -*- coding: utf-8 -*-
"""SegundoIntento-NPL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T4VxD2unkNkIThOTBLdE7euiS82-1nIp
"""

!pip install newspaper3k lxml_html_clean

!pip uninstall -y numpy
!pip install numpy==1.26.4
import numpy as np
print(np.__version__)

import pandas as pd
import matplotlib.pyplot as mpl
import seaborn as sb
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import json

#Helper libraries
import multiprocessing
import numpy as np
import pandas as pd
import math
from bs4 import BeautifulSoup
import re

#Importaciones para algoritmos.
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, RobertaConfig
from sklearn.model_selection import train_test_split
from transformers import EarlyStoppingCallback
from transformers import get_linear_schedule_with_warmup
from torch.optim import AdamW
from datasets import Dataset
import torch

from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics import precision_score, recall_score

# Acceso a nuevos articulos
from newspaper import Article
from bs4 import BeautifulSoup
import re

print(np.__version__)

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
import os

# Montar Google Drive
drive.mount('/content/drive')

# Definir la ruta local en Drive (podés cambiar el nombre de la carpeta final)
drive_base_path = "/content/drive/MyDrive/modelo_bias_politico"

# Crear la carpeta si no existe
os.makedirs(f"{drive_base_path}/results", exist_ok=True)
os.makedirs(f"{drive_base_path}/logs", exist_ok=True)

# Confirmar que todo esté bien
print(f"Modelos y logs se guardarán en: {drive_base_path}")

df = pd.read_csv("hf://datasets/Faith1712/Allsides_political_bias_proper/allsides_data_unstructured.zip")
# 0 --> Izquierda. 1 --> Centro. 2 --> Derecha.
df.shape

df.head()

bias_labels = {0: 'Izquierda', 1: 'Centro', 2: 'Derecha'}
df['bias_label'] = df['label'].map(bias_labels)

#Grafico de distribucion de labels.
mpl.figure(figsize=(8, 5))
sb.countplot(data=df, x='bias_label', palette='Set2', order=['Izquierda', 'Centro', 'Derecha'])
mpl.title("Cantidad de artículos por inclinación política")
mpl.xlabel("Inclinación política")
mpl.ylabel("Cantidad de artículos")
mpl.tight_layout()
mpl.show()

#Palabras mas frecuentes por inclinacion
for label in ['Izquierda', 'Centro', 'Derecha']:
    text = ' '.join(df[df['bias_label'] == label]['text'].dropna().astype(str))
    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Set2').generate(text)

    mpl.figure(figsize=(10, 5))
    mpl.imshow(wordcloud, interpolation='bilinear')
    mpl.axis("off")
    mpl.title(f"Nube de palabras - {label}")
    mpl.tight_layout()
    mpl.show()

vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)

top_n = 10  # cantidad de palabras top a mostrar
top_words_dict = {}

for label in ['Izquierda', 'Centro', 'Derecha']:
    texts = df[df['bias_label'] == label]['text']
    tfidf_matrix = vectorizer.fit_transform(texts)

    # Sumar TF-IDF de cada palabra en todos los documentos del grupo
    tfidf_sum = tfidf_matrix.sum(axis=0)
    tfidf_scores = tfidf_sum.A1  # convertir matriz a array 1D

    # Crear dataframe con palabras y sus scores
    words_scores = pd.DataFrame({
        'word': vectorizer.get_feature_names_out(),
        'score': tfidf_scores
    })

    # Ordenar y seleccionar top palabras
    top_words = words_scores.sort_values(by='score', ascending=False).head(top_n)
    top_words_dict[label] = top_words

    print(f"\nTop {top_n} palabras TF-IDF para {label}:")
    print(top_words)

for label, top_words in top_words_dict.items():
    mpl.figure(figsize=(10, 6))
    sb.barplot(x='score', y='word', data=top_words, palette='Set2')
    mpl.title(f"Top {top_n} palabras TF-IDF para {label}")
    mpl.xlabel("TF-IDF acumulado")
    mpl.ylabel("Palabras")
    mpl.tight_layout()
    mpl.show()

#Observamos que claramente no podremos hacer clasificaciones segun medidas convencionales como tf-idf.
#Por ende sabemos que deberiamos trabajar con elementos como bert o sus derivados.

# Trabajos en la division de datos.
# Primero: train (70%) y temp (30%)
train_texts, temp_texts, train_labels, temp_labels = train_test_split(
    df['text'].tolist(), df['label'].tolist(), test_size=0.3, random_state=42, stratify=df['label']
)

# Segundo: valid (15%) y test (15%) a partir del 30% restante
val_texts, test_texts, val_labels, test_labels = train_test_split(
    temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels
)

#Convertimos nuestros datasets en datasets aptos para huggingface ya que sus componentes son mas eficientes con ellos.
train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})
val_dataset = Dataset.from_dict({'text': val_texts, 'label': val_labels})
test_dataset = Dataset.from_dict({'text': test_texts, 'label': test_labels})

#Configuramos la tokenizacion de Roberta, un modelo derivado de Bert que se desempeña mucho mejor en tareas de clasificación como la nuestra.
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

def tokenize(batch):
    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=512)

train_dataset = train_dataset.map(tokenize, batched=True)
val_dataset = val_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

# Establecemos el formato correcto como tensores de PyTorch
for ds in [train_dataset, val_dataset, test_dataset]:
    ds.set_format(
        type='torch',
        columns=['input_ids', 'attention_mask', 'label'],
        output_all_columns=False  # Importante para evitar columnas innecesarias que puedan causar conflictos
    )

#Instanciamos el modelo que adopataremos para esta tarea.
model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=3)
#Visualizamos el modelo generado
def print_model_structure(module, indent=0):
    """Imprime recursivamente la estructura del modelo."""
    for name, child in module.named_children():
        print("  " * indent + f"({name}): {child.__class__.__name__}")
        # Si es un módulo compuesto, lo exploramos
        print_model_structure(child, indent + 1)

# Mostramos la arquitectura de forma jerárquica
print("Estructura del modelo RoBERTa para clasificación:\n")
print_model_structure(model)

# Congelamos capas 0 a 10 del encoder de RoBERTa
for name, param in model.named_parameters():
    if any(f"encoder.layer.{i}." in name for i in range(9)):
        param.requires_grad = False
    else:
        param.requires_grad = True

# Aseguramos que los embeddings también se entrenen
for param in model.roberta.embeddings.parameters():
    param.requires_grad = True

# Verificamos qué parámetros se van a entrenar
trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]
print(f"\nCapas que se entrenarán ({len(trainable_params)}):")
for name in trainable_params:
    print("  ✓", name)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')
    precision = precision_score(labels, predictions, average='weighted', zero_division=0)
    recall = recall_score(labels, predictions, average='weighted')

    return {
        'accuracy': accuracy,
        'f1_weighted': f1,
        'precision_weighted': precision,
        'recall_weighted': recall,
    }

# 1. Define los argumentos de entrenamiento
training_args = TrainingArguments(
    # Para guardar en Google Drive, usa la ruta definida en drive_base_path
    output_dir=f"{drive_base_path}/results",  # Directorio donde se guardarán los checkpoints y el modelo final
    num_train_epochs=5,      # Número de épocas de entrenamiento
    per_device_train_batch_size=16, # Tamaño del batch por dispositivo durante el entrenamiento
    per_device_eval_batch_size=16,  # Tamaño del batch por dispositivo durante la evaluación
    warmup_steps=500,        # Número de pasos para el warmup del learning rate
    weight_decay=0.01,
    learning_rate=2e-5,# Decaimiento de peso para la regularización L2
    logging_strategy="epoch",
    logging_dir=f"{drive_base_path}/logs",    # Directorio para los logs de TensorBoard en Drive
    logging_steps=500,        # Frecuencia de logueo
    eval_strategy="epoch", # Evaluar al final de cada época
    save_strategy="epoch",   # Guardar el modelo al final de cada época
    load_best_model_at_end=True, # Cargar el mejor modelo al final del entrenamiento
    metric_for_best_model="eval_loss", # Métrica para determinar el mejor modelo
    greater_is_better=False, # Para eval_loss, un valor menor es mejor
)

# 2. Crea el Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer, # Es importante pasar el tokenizer al Trainer
    compute_metrics=compute_metrics
)

# 3. Entrena el modelo
trainer.train()

# 4. Guarda el modelo entrenado
# Guarda el modelo completo (configuración + pesos) en Google Drive
trainer.save_model(f"{drive_base_path}/modelo_sesgo_politico_roberta")
tokenizer.save_pretrained(f"{drive_base_path}/modelo_sesgo_politico_roberta")
print(f"Modelo guardado en: {drive_base_path}/modelo_sesgo_politico_roberta")

#Pasemos a graficar los resultados.
# Accedé a los logs de entrenamiento guardados por Trainer
logs = trainer.state.log_history

# Convertimos a DataFrame
df_logs = pd.DataFrame(logs)

# Filtramos solo las filas con evaluación
eval_logs = df_logs[df_logs["eval_loss"].notnull()]
train_logs = df_logs[df_logs["loss"].notnull()]

mpl.figure(figsize=(12, 5))

# Subplot 1: Loss
mpl.subplot(1, 2, 1)
mpl.plot(train_logs["epoch"], train_logs["loss"], label="Train Loss")
mpl.plot(eval_logs["epoch"], eval_logs["eval_loss"], label="Validation Loss")
mpl.xlabel("Época")
mpl.ylabel("Pérdida")
mpl.title("Pérdida durante el entrenamiento")
mpl.legend()

# Subplot 2: Accuracy
mpl.subplot(1, 2, 2)
mpl.plot(eval_logs["epoch"], eval_logs["eval_accuracy"], marker='o', color='green', label="Validation Accuracy")
mpl.xlabel("Época")
mpl.ylabel("Accuracy")
mpl.title("Accuracy de validación")
mpl.legend()

mpl.tight_layout()
mpl.show()

#Ahora armaremos una matriz de confusion para ver mejor los resultados de la validation.
pred_output = trainer.predict(val_dataset)
logits = pred_output.predictions
true_labels = pred_output.label_ids
pred_labels = np.argmax(logits, axis=1)

bias_labels = ['Izquierda', 'Centro', 'Derecha']

# Matriz de confusión
cm = confusion_matrix(true_labels, pred_labels)

# Graficar matriz de confusión
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=bias_labels)

mpl.figure(figsize=(6, 5))
disp.plot(cmap='Blues', values_format='d')
mpl.title("Matriz de Confusión - Validación")
mpl.tight_layout()
mpl.show()

# Evaluar en test
final_metrics = trainer.evaluate(test_dataset)

# Mostrar métricas
print("\n📊 Métricas del modelo final en conjunto de test:")
for metric, value in final_metrics.items():
    if isinstance(value, float):
        print(f"{metric}: {value:.4f}")

# === 2. MÉTRICAS FINALES SOBRE EL SET DE TEST ===
test_output = trainer.predict(test_dataset)
logits_test = test_output.predictions
true_test_labels = test_output.label_ids
pred_test_labels = np.argmax(logits_test, axis=1)

# Métricas numéricas
acc = accuracy_score(true_test_labels, pred_test_labels)
f1 = f1_score(true_test_labels, pred_test_labels, average='weighted')
precision = precision_score(true_test_labels, pred_test_labels, average='weighted', zero_division=0)
recall = recall_score(true_test_labels, pred_test_labels, average='weighted')

print("\n📊 Métricas finales sobre el conjunto de TEST:")
print(f"Accuracy:  {acc:.4f}")
print(f"F1 Score:  {f1:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")

# Matriz de confusión
cm_test = confusion_matrix(true_test_labels, pred_test_labels)

# Graficar
disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=bias_labels)
mpl.figure(figsize=(6, 5))
disp_test.plot(cmap='Purples', values_format='d')
mpl.title("Matriz de Confusión - Test")
mpl.tight_layout()
mpl.show()

# Calcular precision por clase
precision_por_clase = precision_score(true_test_labels, pred_test_labels, average=None, zero_division=0)

# Etiquetas de clase
clases = ['Izquierda', 'Centro', 'Derecha']

# Crear DataFrame para graficar
df_precision = pd.DataFrame({
    "Clase": clases,
    "Precision": precision_por_clase
})

# Graficar
mpl.figure(figsize=(8, 5))
sb.barplot(x="Clase", y="Precision", data=df_precision, palette="Purples_d")
mpl.ylim(0, 1.0)
mpl.title("Precisión por clase - Conjunto de Test")
mpl.ylabel("Precisión")
mpl.xlabel("Clase")
for i, row in df_precision.iterrows():
    mpl.text(i, row.Precision + 0.005, f"{row.Precision:.3f}", ha='center', va='bottom', fontsize=10)
mpl.tight_layout()
mpl.show()

recall_por_clase = recall_score(true_test_labels, pred_test_labels, average=None)
f1_por_clase = f1_score(true_test_labels, pred_test_labels, average=None)

# DataFrame combinado
df_metrics = pd.DataFrame({
    "Clase": clases,
    "Precision": precision_por_clase,
    "Recall": recall_por_clase,
    "F1-Score": f1_por_clase
})

# Gráfico combinado (opcional)
df_metrics.set_index("Clase").plot(kind="bar", figsize=(10, 6), colormap="Set2")
mpl.ylim(0, 1.0)
mpl.title("Métricas por clase - Test")
mpl.ylabel("Valor")
mpl.xlabel("Clase")
mpl.xticks(rotation=0)
for i, clase in enumerate(df_metrics["Clase"]):
    for j, metric in enumerate(["Precision", "Recall", "F1-Score"]):
        value = df_metrics.loc[i, metric]
        mpl.text(i + j * 0.25 - 0.25, value + 0.005, f"{value:.3f}", ha='center', fontsize=9)
mpl.tight_layout()
mpl.show()

# Crear DataFrame con resultados del test
results_df = pd.DataFrame({
    "Texto": test_texts,
    "Etiqueta_Real": true_test_labels,
    "Etiqueta_Predicha": pred_test_labels
})

# Guardar en Google Drive
results_path = f"{drive_base_path}/results/resultados_test.csv"
results_df.to_csv(results_path, index=False, encoding="utf-8")
print(f"\n✅ Resultados de test guardados en: {results_path}")

# Diccionario con métricas finales
metrics_dict = {
    "accuracy": round(acc, 4),
    "f1_score": round(f1, 4),
    "precision": round(precision, 4),
    "recall": round(recall, 4)
}

# Guardar como JSON
metrics_path = f"{drive_base_path}/results/metricas_test.json"
with open(metrics_path, 'w') as f:
    json.dump(metrics_dict, f, indent=4)

print(f"✅ Métricas finales guardadas en: {metrics_path}")

model_path = drive_base_path+"/modelo_sesgo_politico_roberta"
model = RobertaForSequenceClassification.from_pretrained(model_path)
tokenizer = RobertaTokenizer.from_pretrained(model_path)
model

"""# Agregamos funcion para importar articulos propios (siempre en ingles y que hablen de los estados unidos)"""

def clean(text):
    text = BeautifulSoup(text, "lxml").text
    text = re.sub(r'\|\|\|', r' ', text)
    text = text.replace('„','')
    text = text.replace('\n',' ')
    return text

def predecir_sesgo(url, model, tokenizer, label_names):
    # Descargar y limpiar artículo
    article = Article(url)
    article.download()
    article.parse()
    cleaned_text = clean(article.text)

    # Tokenización
    inputs = tokenizer(cleaned_text,
                       truncation=True,
                       padding=True,
                       return_tensors="pt").to(model.device)

    # Predicción
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=1).item()

    # Mostrar resultado
    return label_names[predicted_class]

url_1 = 'https://nypost.com/2025/06/09/opinion/its-a-small-part-of-la-is-latest-media-refrain-to-minimize-violent-mobs/' #Ejemplo derecha

prediccion = predecir_sesgo(url_1, model, tokenizer, bias_labels)
print("El sesgo político del artículo es:", prediccion)

url_2 = 'https://abcnews.go.com/US/trump-la-siege-mayor-governor-paint-picture/story?id=122652268' #Ejemplo Izquierda

prediccion = predecir_sesgo(url_2, model, tokenizer, bias_labels)
print("El sesgo político del artículo es:", prediccion)

url_3 = 'https://www.bbc.com/news/articles/c78e7gjz387o'

prediccion = predecir_sesgo(url_3, model, tokenizer, bias_labels) #Ejemplo Centro
print("El sesgo político del artículo es:", prediccion)